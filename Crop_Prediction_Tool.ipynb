{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nizrQk06qZsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e71ef3-716b-4716-b092-2ba0ea1e342e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ System Ready.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install & Fix Crash\n",
        "!pip install -q gradio pandas scikit-learn google-generativeai joblib nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# üõ†Ô∏è APPLY FIX: This prevents the \"loop_factory\" error in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Force asyncio to use a compatible runner\n",
        "_original_run = asyncio.run\n",
        "def _patched_run(main, *, loop_factory=None, **kwargs):\n",
        "    return _original_run(main, **kwargs)\n",
        "asyncio.run = _patched_run\n",
        "\n",
        "print(\"‚úÖ System Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Train Models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# 1. Load Data\n",
        "df = pd.read_csv('crop_yield_dataset (1).csv')\n",
        "df['Irrigation'] = df['Irrigation'].fillna('Unknown')\n",
        "df['Previous_Crop'] = df['Previous_Crop'].fillna('Unknown')\n",
        "\n",
        "# 2. Create Synthetic Price (Since it's missing)\n",
        "# Logic: Lower Yield = Higher Price (Supply & Demand)\n",
        "df['Price'] = (50000 / df['Yield_ton_per_ha']) + np.random.normal(0, 200, len(df))\n",
        "\n",
        "# 3. Setup Training\n",
        "X = df.drop(columns=['Yield_ton_per_ha', 'Price'])\n",
        "y_yield = df['Yield_ton_per_ha']\n",
        "y_price = df['Price']\n",
        "\n",
        "# 4. Define Transformers (Handle Text & Numbers)\n",
        "prep = ColumnTransformer([\n",
        "    ('num', StandardScaler(), ['Soil_pH', 'Rainfall_mm', 'Temperature_C', 'Humidity_pct', 'Fertilizer_Used_kg', 'Pesticides_Used_kg', 'Planting_Density']),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), ['Crop', 'Region', 'Soil_Type', 'Irrigation', 'Previous_Crop'])\n",
        "])\n",
        "\n",
        "# 5. Train!\n",
        "yield_model = Pipeline([('p', prep), ('m', RandomForestRegressor(n_estimators=20))])\n",
        "price_model = Pipeline([('p', prep), ('m', RandomForestRegressor(n_estimators=20))])\n",
        "\n",
        "yield_model.fit(X, y_yield)\n",
        "price_model.fit(X, y_price)\n",
        "\n",
        "# 6. Save Options for Dropdowns\n",
        "opts = {col: df[col].unique().tolist() for col in ['Crop', 'Region', 'Soil_Type', 'Irrigation', 'Previous_Crop']}\n",
        "\n",
        "print(\"‚úÖ Models Trained.\")"
      ],
      "metadata": {
        "id": "iORC6nLyqgSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32b923d-2797-4517-c5cb-3757102e2f72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Models Trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define Prediction Logic\n",
        "def predict(crop, region, soil, irrig, prev, ph, rain, temp, humid, fert, pest, dens):\n",
        "    # Create a single row of data\n",
        "    row = pd.DataFrame([{\n",
        "        'Crop': crop, 'Region': region, 'Soil_Type': soil,\n",
        "        'Irrigation': irrig, 'Previous_Crop': prev,\n",
        "        'Soil_pH': ph, 'Rainfall_mm': rain, 'Temperature_C': temp,\n",
        "        'Humidity_pct': humid, 'Fertilizer_Used_kg': fert,\n",
        "        'Pesticides_Used_kg': pest, 'Planting_Density': dens\n",
        "    }])\n",
        "\n",
        "    # Get answers\n",
        "    y_pred = yield_model.predict(row)[0]\n",
        "    p_pred = price_model.predict(row)[0]\n",
        "\n",
        "    return f\"{y_pred:.2f} tons/ha\", f\"‚Çπ {p_pred:.2f} per ton\""
      ],
      "metadata": {
        "id": "3CQN4s75qmku"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Connect Groq AI Chatbot (Ultra Fast)\n",
        "!pip install -q groq\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "# üëá PASTE YOUR GROQ API KEY HERE üëá\n",
        "GROQ_API_KEY = \"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\"\n",
        "\n",
        "if GROQ_API_KEY and GROQ_API_KEY.startswith(\"gsk_\"):\n",
        "    try:\n",
        "        client = Groq(api_key=GROQ_API_KEY)\n",
        "        # Using Llama-3-70b or 8b for instant responses\n",
        "        MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "        print(\"‚úÖ Groq AI Connected! (Llama-3)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Connection Error: {e}\")\n",
        "        client = None\n",
        "else:\n",
        "    client = None\n",
        "    print(\"‚ö†Ô∏è No valid Groq Key detected in Cell 4.\")\n",
        "\n",
        "def chat(msg, history):\n",
        "    if not client:\n",
        "        return \"‚ö†Ô∏è Chatbot is offline. Please add your Groq API Key in Cell 4.\"\n",
        "\n",
        "    # Using your actual dataset options to guide the AI\n",
        "    system_prompt = f\"You are an expert Agri-Advisor. Use this dataset context: {opts}.\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": msg}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=512\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Groq Error: {e}\")\n",
        "        return \"Groq is currently busy or the key is incorrect. Please try again.\""
      ],
      "metadata": {
        "id": "cB7z-vJpqq-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fff5d2-a77e-46a9-973c-9deed45e5552"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/138.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m133.1/138.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Groq AI Connected! (Llama-3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Launch UI\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## üöú SmartHarvest Engine\")\n",
        "\n",
        "    with gr.Tab(\"Predictor\"):\n",
        "        # Inputs\n",
        "        with gr.Row():\n",
        "            c1 = gr.Dropdown(opts['Crop'], label=\"Crop\", value=\"Rice\")\n",
        "            c2 = gr.Dropdown(opts['Region'], label=\"Region\")\n",
        "            c3 = gr.Dropdown(opts['Soil_Type'], label=\"Soil\")\n",
        "            c4 = gr.Dropdown(opts['Irrigation'], label=\"Irrigation\")\n",
        "            c5 = gr.Dropdown(opts['Previous_Crop'], label=\"Prev Crop\")\n",
        "\n",
        "        with gr.Row():\n",
        "            n1 = gr.Slider(4,9, label=\"pH\"); n2 = gr.Slider(0,2000, label=\"Rain\")\n",
        "            n3 = gr.Slider(10,50, label=\"Temp\"); n4 = gr.Slider(0,100, label=\"Humidity\")\n",
        "            n5 = gr.Number(100, label=\"Fertilizer\"); n6 = gr.Number(20, label=\"Pest\")\n",
        "            n7 = gr.Number(5000, label=\"Density\")\n",
        "\n",
        "        btn = gr.Button(\"Predict Yield & Price\", variant=\"primary\")\n",
        "        out = [gr.Textbox(label=\"Yield\"), gr.Textbox(label=\"Price\")]\n",
        "\n",
        "        # Connect Button\n",
        "        btn.click(predict, inputs=[c1,c2,c3,c4,c5,n1,n2,n3,n4,n5,n6,n7], outputs=out)\n",
        "\n",
        "    with gr.Tab(\"Chat\"):\n",
        "        gr.ChatInterface(chat)\n",
        "\n",
        "# Launch!\n",
        "demo.launch(share=True, debug=False)"
      ],
      "metadata": {
        "id": "44B-23mfqunT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "806368ca-63d1-48df-ce55-bbc96b6bb5f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2482919974.py:4: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7a1b894a14e965361e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7a1b894a14e965361e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}